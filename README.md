# ETL Streaming com Spark, Kafka e Airflow

O Apache Airflow será utilizado para orquestração enquanto o Kafka será usado para gerenciar o streaming de dados em tempo real. O PySpark será utilizado para processamento distribuído de dados e o resultado do processamento em tempo real será armazenado em um banco de dados NoSQL. Todo o pipeline será criado para funcionar de forma automatizada.
